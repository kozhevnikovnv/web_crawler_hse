<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
--><html class="no-js" lang="ru">
<head>
<meta charset="utf-8"/>
<!-- begin _includes/seo.html --><title>Вывод и предсказания. Часть 1: машинное обучение | Михаил Коротеев</title>
<meta content="Перевод первой из трех статей про сравнение двух подходов к моделированию данных - статистическому и обучающему. Первая часть посвящена анализу алгоримтов машинного обучения для анализа данных. Очень рекомендуется к прочтению всем заинетерсованным. Будет интересна как новичкам, так и продолжающим изучать машинное обучение." name="description"/>
<meta content="Михаил Коротеев" name="author"/>
<meta content="Михаил Коротеев" property="article:author"/>
<meta content="article" property="og:type"/>
<meta content="ru_RU" property="og:locale"/>
<meta content="Михаил Коротеев" property="og:site_name"/>
<meta content="Вывод и предсказания. Часть 1: машинное обучение" property="og:title"/>
<meta content="https://koroteev.site/scipop/inference-and-prediction-1/" property="og:url"/>
<meta content="Перевод первой из трех статей про сравнение двух подходов к моделированию данных - статистическому и обучающему. Первая часть посвящена анализу алгоримтов машинного обучения для анализа данных. Очень рекомендуется к прочтению всем заинетерсованным. Будет интересна как новичкам, так и продолжающим изучать машинное обучение." property="og:description"/>
<meta content="https://koroteev.site/assets/images/android-chrome-192x192.png" property="og:image"/>
<meta content="@koroteev_m" name="twitter:site"/>
<meta content="Вывод и предсказания. Часть 1: машинное обучение" name="twitter:title"/>
<meta content="Перевод первой из трех статей про сравнение двух подходов к моделированию данных - статистическому и обучающему. Первая часть посвящена анализу алгоримтов машинного обучения для анализа данных. Очень рекомендуется к прочтению всем заинетерсованным. Будет интересна как новичкам, так и продолжающим изучать машинное обучение." name="twitter:description"/>
<meta content="https://koroteev.site/scipop/inference-and-prediction-1/" name="twitter:url"/>
<meta content="summary" name="twitter:card"/>
<meta content="https://koroteev.site/assets/images/android-chrome-192x192.png" name="twitter:image"/>
<meta content="2021-01-12T00:00:00+00:00" property="article:published_time"/>
<link href="https://koroteev.site/scipop/inference-and-prediction-1/" rel="canonical"/>
<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Михаил Коротеев",
      "url": "https://koroteev.site/"
    
  }
</script>
<meta content="77706580" name="yandex-verification"/>
<!-- end _includes/seo.html -->
<link href="/feed.xml" rel="alternate" title="Михаил Коротеев Feed" type="application/atom+xml"/>
<!-- https://t.co/dKP3o1e -->
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>
<!-- For all browsers -->
<link href="/assets/css/main.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" rel="stylesheet"/>
<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->
<!-- start custom head snippets -->
<style type="text/css">
.accordion {
  border: 1px solid white;
  padding: 0 10px;
  margin: 0 auto;
  list-style: none outside;
}

.accordion > * + * { border-top: 1px solid white; }

.accordion-item-hd {
  display: block;
  padding: 15px 30px 15px 0;
  position: relative;
  cursor: pointer;
  font-size: 18px;
  font-weight: bold;
}

.accordion-item-input:checked ~ .accordion-item-bd {
  max-height: 1000px;
  padding-top: 15px;
  margin-bottom: 15px;
  -webkit-transition: max-height 1s ease-in, margin .3s ease-in, padding .3s ease-in;
  transition: max-height 1s ease-in, margin .3s ease-in, padding .3s ease-in;
}

.accordion-item-input:checked ~ .accordion-item-hd > .accordion-item-hd-cta {
  -webkit-transform: rotate(0);
  -ms-transform: rotate(0);
  transform: rotate(0);
}

.accordion-item-hd-cta {
  display: block;
  width: 30px;
  position: absolute;
  top: calc(50% - 6px );
  /*minus half font-size*/
  right: 0;
  pointer-events: none;
  -webkit-transition: -webkit-transform .3s ease;
  transition: transform .3s ease;
  -webkit-transform: rotate(-180deg);
  -ms-transform: rotate(-180deg);
  transform: rotate(-180deg);
  text-align: center;
  font-size: 12px;
  line-height: 1;
}

.accordion-item-bd {
  max-height: 0;
  margin-bottom: 0;
  overflow: hidden;
  -webkit-transition: max-height .15s ease-out, margin-bottom .3s ease-out, padding .3s ease-out;
  transition: max-height .15s ease-out, margin-bottom .3s ease-out, padding .3s ease-out;
}

.accordion-item-input {
  clip: rect(0 0 0 0);
  width: 1px;
  height: 1px;
  margin: -1;
  overflow: hidden;
  position: absolute;
  left: -9999px;
}
</style>
<!-- insert favicons. use https://realfavicongenerator.net/ -->
<!-- end custom head snippets -->
</head>
<body class="layout--single None">
<nav class="skip-links">
<h2 class="screen-reader-text">Skip links</h2>
<ul>
<li><a class="screen-reader-shortcut" href="#site-nav">Skip to primary navigation</a></li>
<li><a class="screen-reader-shortcut" href="#main">Skip to content</a></li>
<li><a class="screen-reader-shortcut" href="#footer">Skip to footer</a></li>
</ul>
</nav>
<!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
<div class="masthead">
<div class="masthead__inner-wrap">
<div class="masthead__menu">
<nav class="greedy-nav" id="site-nav">
<a class="site-logo" href="/"><img alt="" src="/assets/images/android-chrome-192x192.png"/></a>
<a class="site-title" href="/">
          Михаил Коротеев
          <span class="site-subtitle">Личный сайт</span>
</a>
<ul class="visible-links"><li class="masthead__menu-item">
<a href="/os">UNIX</a>
</li><li class="masthead__menu-item">
<a href="/ml">ML</a>
</li><li class="masthead__menu-item">
<a href="/md">Android</a>
</li><li class="masthead__menu-item">
<a href="/courses">Все курсы</a>
</li><li class="masthead__menu-item">
<a href="/blog/">Блог</a>
</li><li class="masthead__menu-item">
<a href="/science">Проекты</a>
</li></ul>
<button class="greedy-nav__toggle hidden" type="button">
<span class="visually-hidden">Выпадающее меню</span>
<div class="navicon"></div>
</button>
<ul class="hidden-links hidden"></ul>
</nav>
</div>
</div>
</div>
<div class="initial-content">
<div id="main" role="main">
<div class="sidebar sticky">
<div itemscope="" itemtype="https://schema.org/Person">
<div class="author__avatar">
<img alt="Михаил Коротеев" itemprop="image" src="/assets/images/avatar3.jpg"/>
</div>
<div class="author__content">
<h3 class="author__name" itemprop="name">Михаил Коротеев</h3>
<div class="author__bio" itemprop="description">
<p>Есть такая профессия - Родину автоматизировать.</p>
</div>
</div>
<div class="author__urls-wrapper">
<button class="btn btn--inverse">Связаться со мной</button>
<ul class="author__urls social-icons">
<li><a href="https://koroteev.site" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fas fa-fw fa-link"></i><span class="label">Website</span></a></li>
<li><a href="https://github.com/koroteevmv" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fab fa-fw fa-github"></i><span class="label">GitHub</span></a></li>
<li><a href="https://www.youtube.com/channel/UCPe6h6MY8XZpLV7oRLqRUwQ" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fab fa-fw fa-youtube"></i><span class="label">YouTube</span></a></li>
<li><a href="https://vk.com/koroteev_m" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fab fa-fw fa-vk"></i><span class="label">Vkontakte</span></a></li>
<!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
</ul>
</div>
</div>
</div>
<article class="page" itemscope="" itemtype="https://schema.org/CreativeWork">
<meta content="Вывод и предсказания. Часть 1: машинное обучение" itemprop="headline"/>
<meta content="Перевод первой из трех статей про сравнение двух подходов к моделированию данных - статистическому и обучающему. Первая часть посвящена анализу алгоримтов машинного обучения для анализа данных. Очень рекомендуется к прочтению всем заинетерсованным. Будет интересна как новичкам, так и продолжающим изучать машинное обучение." itemprop="description"/>
<meta content="2021-01-12T00:00:00+00:00" itemprop="datePublished"/>
<div class="page__inner-wrap">
<header>
<h1 class="page__title" id="page-title" itemprop="headline">Вывод и предсказания. Часть 1: машинное обучение
</h1>
<p class="page__meta">
<span class="page__meta-date">
<i aria-hidden="true" class="far fa-calendar-alt"></i>
<time datetime="2021-01-12T00:00:00+00:00">January 12, 2021</time>
</span>
<span class="page__meta-sep"></span>
<span class="page__meta-readtime">
<i aria-hidden="true" class="far fa-clock"></i>
        
          15 мин на чтение
        
                
      </span>
</p>
</header>
<section class="page__content" itemprop="text">
<aside class="sidebar__right sticky">
<nav class="toc">
<header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Содержание</h4></header>
<ul class="toc__menu">
<li><a href="#пример-моделирование-доли-пользовательского-отклика">Пример: моделирование доли пользовательского отклика</a></li>
<li><a href="#взгляд-на-данные">Взгляд на данные</a></li>
<li><a href="#предсказание---подход-машинного-обучения">Предсказание - подход машинного обучения</a>
<ul>
<li><a href="#сборка-модели-воедино">Сборка модели воедино</a></li>
<li><a href="#обучение-из-машинного-обучения">“Обучение” из машинного обучения</a></li>
<li><a href="#обоснование-отрицательного-логарифмического-правдоподобия">Обоснование отрицательного логарифмического правдоподобия</a></li>
<li><a href="#оптимизация---старый-добрый-градиентный-спуск">Оптимизация - старый добрый градиентный спуск</a></li>
<li><a href="#статистика-но-ведь-это-всего-лишь">Статистика: “Но ведь это всего лишь…”</a></li>
<li><a href="#измерение-эффективности-модели-машинного-обучения">Измерение эффективности модели машинного обучения</a></li>
<li><a href="#ограничения-машинного-обучения">Ограничения машинного обучения</a></li>
<li><a href="#проблема-предсказания">Проблема предсказания</a></li>
</ul>
</li>
</ul>
</nav>
</aside>
<p><a href="https://www.countbayesie.com/blog/2020/12/15/inference-and-prediction-part-1-machine-learning">Оригинал</a></p>
<p>Это первая статья из трех, посвященных разнице между выводом и предсказанием в моделировании данных. Также мы разберем разницу между <strong>машинным обучением</strong> и <strong>статистикой</strong>. За мою карьеру аналитика данных я обнаружил удивительное непонимание понятия вывода, который обычно понимается как часть статистики. Я также понял, что, хотя это менее распространено, профессиональные статистики не очень хорошо понимают, как машинное обучение ставит задачи предсказания.</p>
<p>Немногим более года назад я запостил картинку в Твиттере, которая отражает мое понимание этих понятий:</p>
<p><img alt="alt_text" src="/assets/images/inference-and-prediction-1/images/image1.jpg" title="image_tooltip"/></p>
<p>В этой серии статей мы увидим, что эти два понятия связаны больше, чем обычно представляют.</p>
<p>В первой статье я бы хотел не только наметить общую идею через рабочий пример, но и попытаться сблизить эти два понятия. Конечной целью каждого исследователя должно быть рассмотрение моделирование как системного процесса, использующего все возможности математических рассуждений. Процесс моделирования должен быть интерактивным, выходящим за рамки этих двух понятий, что особенно актуально в наш век доступных вычислительных возможностей.</p>
<h2 id="пример-моделирование-доли-пользовательского-отклика">Пример: моделирование доли пользовательского отклика</h2>
<p>Одна из самых распространенных проблем в индустрии - это моделирование доли пользовательского отклика (Click Through Rate, CTR). Обычно проблема CTR возникает, когда мы оцениваем выполнение пользователем определенных действий: подписка на сервис через форму, щелчки по рекламе, просмотр описания товаров в каталоге, покупка товаров, чтение постов в агрегаторе, и так далее.</p>
<p>В предыдущей статье мы говорили о <a href="https://www.countbayesie.com/blog/2020/9/26/learn-thompson-sampling-by-building-an-ad-auction">процессе сэмплирования Томпсона при оптимизации рекламных аукционов</a>. Одна из ключевых частей той статьи была посвящена оценке CTR с помощью бета-распределения, зная только предыдущее соотношение кликов и просмотров. И хотя этот подход прекрасно работает, обычно у нас есть много дополнительной информации, которую можно использовать для более точного понимания пользовательского отклика.</p>
<p>В этом примере мы будем использовать информацию об откликах пользователей на объявления о вакансиях с сайта объявлений. Данные взяты из этого<a href="https://www.kaggle.com/animeshgoyal9/predict-click-through-rate-ctr-for-a-website"> набора с Kaggle</a>. Мы используем эти данные для моделирования того, откликнется ли пользователь на данное объявление о вакансии, исходя из его категории, и того, насколько соответствует заголовок объявления пользовательской строке запроса.</p>
<p>На протяжении всей моей карьеры в анализе данных меня удивляет то, насколько часто эти задачи трактуются как чисто предсказательные. Как мы увидим в этой статье, исключительно предсказательный подход к проблеме CTR приводит к ограниченности того, как можно решать разные задачи, относящиеся к этой проблеме (а именно, как повысить отклики!). Для начала мы подойдем к этой задаче с точки зрения машинного обучения, построив простой персептрон на Python и JAX, а затем, в следующей части, расширим эту модель для решения задачи вывода.</p>
<h2 id="взгляд-на-данные">Взгляд на данные</h2>
<p>Прежде чем мы начнем, взглянем на очищенные данные из нашего набора. Вот какие признаки мы будем использовать:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'main_query_tfidf'</span><span class="p">,</span> <span class="s">'query_jl_score'</span><span class="p">,</span>
            <span class="s">'query_title_score'</span><span class="p">,</span><span class="s">'job_age_days'</span><span class="p">,</span>
            <span class="s">'job_16140'</span><span class="p">,</span> <span class="s">'job_31542'</span><span class="p">,</span><span class="s">'job_41757'</span><span class="p">,</span>
            <span class="s">'job_42467'</span><span class="p">,</span><span class="s">'job_45300'</span><span class="p">,</span><span class="s">'job_51966'</span><span class="p">,</span>
            <span class="s">'job_67237'</span><span class="p">,</span> <span class="s">'job_69982'</span><span class="p">,</span> <span class="s">'job_77312'</span><span class="p">,</span>
            <span class="s">'job_82238'</span><span class="p">,</span><span class="s">'job_other'</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Первые три признака описывают разные метрики близости строки поиска пользователя и заголовка объявления о вакансии, следующий - возраст объявления в днях, а затем у нас идут 10 самых распространенных категорий объявлений как индикация того, принадлежит ли данное объявление к этой категории.</p>
<p>Вот как выглядят данные, уже приведенные к стандартному нормальному распределению:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">apply_data</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p><img alt="alt_text" src="/assets/images/inference-and-prediction-1/images/image9.png" title="image_tooltip"/></p>
<p>Как Вы видите, у нас достаточно данных - 1 200 890 строк.</p>
<p>Далее мы будем производить множество вычислений по этим данным, так что приведем их в формат матрицы jax и разделим на обучающую и тестовую выборки:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">apply_data</span><span class="p">[</span><span class="n">features</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">apply_data</span><span class="p">[</span><span class="s">'apply'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="mf">0.7</span><span class="o">*</span><span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">train_size</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Теперь можем начинать моделирование!</p>
<h2 id="предсказание---подход-машинного-обучения">Предсказание - подход машинного обучения</h2>
<p>Теперь когда мы полностью подготовили данные, время заняться моделированием! Если вы опытный аналитик данных или эксперт по машинному обучению, большая часть этой статьи будет вам знакома. Это намеренно, так как я хочу показать логику, которая связывает традиционных взгляд с точки зрения машинного обучения со взглядом статистическим. Так что, хотя тема и знакома, стоит повторить, как именно мы думаем о решении проблем машинного обучения.</p>
<p>Так как мы занимаемся машинным обучением, хочется начать с нейронной сети. Но мы хотим начать с чего-то разумно простого, так что выберем самый простой тип нейронных сетей: персептрон. Если вы незнакомы с персептроном - это просто нейронная сеть без скрытых слоев. Вот иллюстрация нашей модели:</p>
<p><img alt="alt_text" src="/assets/images/inference-and-prediction-1/images/image2.png" title="image_tooltip"/></p>
<p>Обратите внимание на элемент “bias” или “смещение” на картинке. Мы можем его представить, добавив постоянный признак к нашему набору данных, что я уже сделал с массивами X_train и X_test.</p>
<h3 id="сборка-модели-воедино">Сборка модели воедино</h3>
<p>Давайте подумаем о математике, лежащей в основе нашей модели. У нас есть графическое представление того, как устроен персептрон, на нам нужно понять, как заставить его работать в коде. У нас есть массивы X и y, то есть, соответственно, данные и целевая переменная. Нужно добавить кое-то еще.</p>
<p>Первый шаг - это представление всех линий на нашей диаграмме численно. Мы сделаем это с помощью вектора весов <em>w</em>. <em>Но какие значения должны быть у этого вектора?</em> Вот их как раз мы и хотим подобрать, или, как говорят, обучить (точнее мы хотим, чтобы машина их обучила).</p>
<p>Мы опять используем для этого jax, и для начала сгенерируем случайные веса. В отличие от обычного numpy, jax всегда требует ключ для генерации случайных значений. Это значит, что случайные переменные будут предсказуемыми. Это очень полезно при отладке нашей модели, ведь мы будем производить много случайных действий, а так они могут быть воспроизведены в точности. Вот пример того, как можно создать массив случайных весов:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1337</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> 
                   <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> 
                   <span class="n">minval</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span>
                   <span class="n">maxval</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Линии на диаграмме соответствуют перемножению значений признаков данных с весами и их суммированию. Математически мы можем представить перемножение и суммирование очень просто при помощи линейной алгебры:</p>

\[X w\]

<p>В коде это тоже довольно просто получается:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">DeviceArray</span><span class="p">([</span><span class="mf">0.15793219</span><span class="p">,</span> <span class="mf">0.07844558</span><span class="p">,</span> <span class="mf">0.07844558</span><span class="p">,</span> <span class="p">...,</span> <span class="mf">0.27550998</span><span class="p">,</span>
             <span class="mf">0.10248479</span><span class="p">,</span> <span class="mf">0.13946426</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Но мы еще не закончили. Есть одна заметная проблема. Мы хотим предсказывать 1 (если пользователь откликнется на вакансию) или 0 (если нет), так что нам нужен способ убедиться, что все наши предсказания находятся в этом диапазоне. Для этого нам понадобится нелинейная функция, которая сожмет получаемые значения в диапазон от 0 до 1. Используя g для обозначения этой функции, мы можем представить полное математическое представление персептрона как:</p>

\[y=g(X w)\]

<p>Самым простым вариантом для g будет логистическая функция, которая как раз выдает значения от 0 до 1. Кроме того, выдаваемые ей значения могут быть интерпретированы как вероятности. Мы еще этого коснемся во второй части, но это важно для будущего обучения нашей модели. Вот математическое выражение логистической функции:</p>

\[g(x) = \frac{1}{1+e^{-x}}\]

<p>А вот то же самое в коде:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">jnp</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">val</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Можем теперь применить это к X и случайным весам и получим нашу первую модель!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">guesses</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
<span class="n">guesses</span>
<span class="o">&gt;</span> <span class="n">DeviceArray</span><span class="p">([</span><span class="mf">0.5394012</span> <span class="p">,</span> <span class="mf">0.51960135</span><span class="p">,</span> <span class="mf">0.51960135</span><span class="p">,</span> <span class="p">...,</span> <span class="mf">0.5684451</span> <span class="p">,</span>
              <span class="mf">0.52559876</span><span class="p">,</span> <span class="mf">0.53480965</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Это, по сути дела, именно случайные угадывания. Следующий шаг - попробовать улучшить их.</p>
<h3 id="обучение-из-машинного-обучения">“Обучение” из машинного обучения</h3>
<p>Конечно, наша модель пока ничего не знает о мире. Можно заметить, что она почти всегда выдает близке к 0,5 значения, ровно посередине между нашими вариантами 0 и 1. При предсказании мы обычно выбираем (точнее за нас выбирает программный инструмент) 0,5 как пороговое значение между этими вариантами. Так что пока  примерно половина случаев будет определяться как предполагаемое наличие отклика, а половина - как отсутствие. Результат может немного отличаться из-за неравномерности распределения данных, но по сути мы просто умножаем равномерно распределенные случайные значения (веса модели) на значения, приведенные к стандартному распределению. Если подставить среднее значение - 0 в логистическую функцию, как раз и получается 0,5. Естественно, эти веса надо поменять со случайных на такие, которые лучшим образом преобразуют данные в нужный результат.</p>
<p>Можно иронизировать над названием “машинное обучение” для такого простого случая, но он даст нам хорошее описание используемых алгоритмов.  Мы хотим, чтобы <em>машина</em> (то есть компьютер) <em>обучилась</em> лучшим <em>w</em> для нашей модели. Ключ к этому в том, чтобы дать понять машине, что один набор весов лучше или хуже другого. Мы добьемся этого, создав <strong>целевую функцию</strong> (ее еще называют функция <em>потерь</em>), которая определяет, насколько удачны изменения в модели.</p>
<p>Мы уже упоминали, что преимущество использования логистической функции в том, что мы может воспринимать ее значения как вероятности. Так значение 0,2 значит, что вероятность отклика на вакансию составляет 20%, а значение 0,75 - что 75%. Значения целевой переменной тоже так работают. Если мы знаем, что пользователь не ответил на вакансию, мы можем сказать, что вероятность ответа составила 0, а если ответил, то вероятность достоверного события равна 1.</p>
<p>Так можно понять, как сконструировать целевую функцию, которую мы будем оптимизировать. Если модель выдает 0.75, мы можем спросить, “Какова вероятность получения 1, если мы считаем, что вероятность отклика равна 75%?”</p>
<p>Очевидно, это просто 0,75. И наоборот, если модель выдает 0,75 а у нас записано 0, вероятность того, что модель верна всего 25%. Мы хотим получить такую функцию, которая вычисляет, <em>насколько вероятны известные нам исходы при данных значениях весов</em>. Как только мы ее получим, останется только найти веса, которые лучше всего объясняют известные нам данные. Это подход <strong>максимального правдоподобия</strong> для нахождения лучших весов.</p>
<h3 id="обоснование-отрицательного-логарифмического-правдоподобия">Обоснование отрицательного логарифмического правдоподобия</h3>
<p>Пока мы говорили о нахождении правдоподобие единственной точки данных, но нам нужен способ подсчитать это для всех 840 623 строк из обучающей выборки. Вот пример кода, который вычисляет правдоподобие данных при имеющихся весах для всего обучающего набора (p_d_h означает Probability of Data given the Hypothesis, вороятность данных при гипотезе):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">y_prob</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
<span class="n">p_d_h</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_prob</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">p_d_h</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mf">0.46059883</span> <span class="mf">0.48039865</span> <span class="mf">0.51960135</span> <span class="p">...</span> <span class="mf">0.4315549</span>  <span class="mf">0.47440124</span> <span class="mf">0.46519035</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Каждое значение представляет, <em>насколько вероятен наблюдаемый исход, если предположить, что модель верна</em>.</p>
<p>Мы хотим объединить их все в единое значение правдоподобия данных для конкретной модели. Мы могли бы для этого просто перемножить их:</p>

\[P(y \mid X w) = \prod^{N}_{i=1}{P(y_i \mid X w)}\]

<p>Это соответствует полной вероятности всех наблюдений при верности нашей модели.</p>
<p>И хотя это математически корректно, можно быстро увидеть проблему:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">jnp</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">p_d_h</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">DeviceArray</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Так как мы берем произведение более 800 000  вероятностей, полученный результат меньше минимального числа, которое может быть представлено в компьютере. Вероятность случайных весов получилась равна нулю, но даже если бы у нас была прекрасная модель, дающая, скажем, вероятность правильного прогноза 0,99, все равно получилось бы:</p>

\[0,99^{840 623}\]

<p>Что для компьютера все равно 0.</p>
<p>Хорошее решение этой проблемы - это взять логарифм каждой вероятности и суммировать их вместо того, чтобы перемножать. Это даст нам логарифмическое правдоподобие, что гораздо удобнее при использовании компьютера. Как мы видим, логарифмическое правдоподобие считается более содержательно:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">jnp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_d_h</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="n">DeviceArray</span><span class="p">(</span><span class="o">-</span><span class="mf">619300.56</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Осталась одна проблема с этим значением. Обычно, оптимизационные алгоритмы находят минимальное значение выпуклой функции. Чем меньше наше значение, тем ниже правдоподобие модели, а нам нужно наоборот. К счастью, логарифмическое правдоподобие всегда отрицательно (так как обычное значение правдоподобия всегда меньше единицы), так что мы может просто взять противоположное значение.</p>
<p>Вот все это в виде одной функции:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
    <span class="n">y_prob</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
    <span class="n">p_d_h</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_prob</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_d_h</span><span class="p">))</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">ll</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Теперь мы можем посчитать отрицательное логарифмическое правдоподобие имеющихся данных для наших весов, еще до начала какого-либо обучения:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">neg_log_likelihood</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">DeviceArray</span><span class="p">(</span><span class="mf">619300.56</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Как видно, отрицательное правдоподобие - это положительное число (ведь логарифм числа между 0 и 1 отрицателен). Теперь нам нужно найти такие значения весов <em>w</em>, которые уменьшают это значение, а значит увеличивают вероятность получения из весов известных нам данных.</p>
<h3 id="оптимизация---старый-добрый-градиентный-спуск">Оптимизация - старый добрый градиентный спуск</h3>
<p>Машина уже почти готова обучаться! Полезно думать о весах <em>w</em>, как определенной модели. С помощью отрицательного логарифмического правдоподобия мы можем различать, какая модель лучше или хуже - а именно, когда это значение ниже.</p>
<p>Мы имеем классическую оптимизационную задачу: необходимо последовательно понижать значение отрицательного правдоподобия до тех пор, пока мы не найдем значение, которое покажется нам самым низким. Мы будем использовать метод градиентного спуска - алгоритм нахождения минимума функции при помощи производной.</p>
<p>Так что осталось найти производную функции отрицательного логарифмического правдоподобия. Раньше нахождение градиента было трудной задачей, так как приходилось все производные считать руками. JAX делает эту процедуру тривиальной:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="n">d_nll_wrt_w</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">,</span><span class="n">argnums</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>В этом коде мы получаем производную функции <em>neg_log_likelihood</em> по второму аргументу (нумерация начинается с нуля), то есть по весам <em>w</em>.</p>
<p>Далее мы создадим простую версию алгоритма градиентного спуска. Учитывая долгую популярностей нейронных сетей, существует большое количество учебников, разъясняющих детали этого метода, так что не будем в них вдаваться. Общая идея в том, что при помощи производной можно спускаться по значению функции до тех пор, пока мы не окажемся в локальном минимуме.</p>
<p>Добавим еще один шаг для более эффективного вычисления градиента:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="n">d_nll_wrt_w_c</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">d_nll_wrt_w</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Так как наша реализация обучения весьма схематичная, можно просто повторить градиентный спуск несколько раз до тех пор, пока получаемые значения не сойдутся к чему-нибудь.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">d_nll_wrt_w_c</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">neg_log_likelihood</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mf">251421.27</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Вот теперь мы обучили модель! Теперь надо посмотреть, насколько точно она описывает известные данные о пользовательских откликах на вакансии.</p>
<h3 id="статистика-но-ведь-это-всего-лишь">Статистика: “Но ведь это всего лишь…”</h3>
<p>Хочу сделать небольшое отступление, чтобы обратиться к одному весьма распространенному среди специалистов по статистике заблуждению. Статистики сразу же распознают, что наша реализация персептрона полностью эквивалента логистической регрессии (подробнее об этом во второй части). Распространенное возражение со стороны статистики звучит так:</p>
<p>“Разве это и есть машинное обучение? Это же просто логистическая регрессия, которую мы применяем уже давно, не особо задумываясь обо все этом!”</p>
<p>Я поспорю, что важное отличие подхода машинного обучения в том, что мы действительно задумываемся обо все этом. О процессе оптимизации, о выборе функции потерь, о выборе метода оптимизации - все это <em>неотъемлемая часть процесса моделирования</em>. Например, для обычной линейной регрессии статистики наверняка прибегнут к методу наименьших квадратов. Это аналогично выбору среднеквадратической ошибка в качестве функции потерь, а это этого зависит, что мы можем делать при помощи нашей модели.</p>
<p>При увеличении сложности моделей вопросы о целевой функции и методах оптимизации выходят на первый план. Но даже в таком простом примере как наш функция отрицательного логарифмического правдоподобия была выбрана неслучайно, хотя существует множество способов обучиться практически тем же весам <em>w</em>. В следующих частях эта целевая функция позволит нам расширять модель с минимальным изменением кода.</p>
<p>Мы видим, что сама статистика трансформируется в этом направлении. Почти все работы в передовой области байесовского вывода требуют глубокого понимания алгоритмов оптимизации и численных методов. Другими словами, этот фокус на вычислимости и оптимизации и отличает машинное обучение от статистики.</p>
<h3 id="измерение-эффективности-модели-машинного-обучения">Измерение эффективности модели машинного обучения</h3>
<p>Сейчас полезно остановиться и подумать, что именно мы хотим измерять при оценке эффективности нашей модели. Конечная цель систем машинного обучения - <strong>предсказание</strong> какой-либо величины. Сейчас она выдает число от нуля до единицы, которые и составляет предсказание модели.</p>
<p>Эта модель также является <strong>классификатором</strong>. Мы представляем, что модель предсказывает, откликнется ли пользователь на данное объявление или нет. При построении классификатора мы добавим последнее преобразование, которое и будет решать, выдать 1 или 0 в качестве конечного результата.</p>
<p>Существует множество метрик эффективности моделей. Когда мы имеем дело с классификацией, естественно воспользоваться метрикой точности - отношением правильно сделанных предсказаний к их общему количеству. Но с этим методом есть две сложности. Первая становится очевидной, если мы подсчитаем долю откликов к общему количеству просмотров:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sum</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">/</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;</span> <span class="mf">0.0899083179974852</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Так что у нас здесь присутствует классическая проблема несбалансированных классов (хотя, мне кажется это странным отношением к ситуации, особенно учитывая, что мы моделируем долю отклика). Проблема в том, что просто предсказывая 0 для всех случаев даст весьма неплохую оценку точности.</p>
<p>Другая, менее явная проблема связана с тем, что мы по-настоящему еще не задумывались о том, как выбирать ответ 0 или 1. Почти во всех библиотеках алгоритмов машинного обучения порог выбора задан неявно как 0,5. И это почти всегда настраивается, но я встречал немало озадаченных взглядов от аналитиков после вопроса “какое пороговое значение используется в вашей модели?”. Более формально этот параметр задается минимальным softmax слоем нейронной сети.</p>
<p>Но мы обычно не знаем заранее, какое значение порога даст нам наилучшие предсказания пользовательских откликов, так что мы используем специальную метрику, показывающую, насколько хорошо работает одна и та же модель при разных значениях порога - ROC AUC (эта метрика заслуживает специального обсуждения в отдельной статье).</p>
<p>Если вы с ней незнакомы, ROC AUC вычисляет площадь под графиком функции соотношения истинноположительных и ложноположительных долей предсказаний при различных значениях порога. Обычно, ROC AUC 0,5 означает, что наш классификатор все равно, что угадывает результат, а значение 1,0 свидетельствует об идеальной классификации.</p>
<p>Давайте посмотрим на вычисление метрик обучающих и тестовых данных:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">pred_train</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">pred_train</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mf">0.5758443732443408</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">w</span><span class="p">))</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">.</span><span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mf">0.5751280457175497</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Получилось не хорошо. Такое значение говорит о том, что наш классификатор не может эффективно разделить пользователей, которые откликаются на вакансии от остальных. Может показаться, что наша модель провалилась.</p>
<h3 id="ограничения-машинного-обучения">Ограничения машинного обучения</h3>
<p>Но действительно, так ли она плоха? Давайте для начала посмотрим, как мы обучились. Конечное правдоподобие было -251,422, а в начале - -619,300. Помним, что</p>

\[\frac{A}{B} = e^{log(A) - log(B)}\]

<p>Это значит, что мы улучшили изначальное правдоподобие вот во сколько раз:</p>

\[e^{366878}\]

<p>После обучения наша модель описывает данные лучше настолько, что мы даже не можем выразить коэффициент улучшения на компьютере! Так что мы многому научились.</p>
<p>Действительная проблема в том, как мы судим нашу модель. Точность, AUC, полнота, F1 - это все распространенные метрики для <em>классификаторов</em>. А мы в самом начале говорили о моделировании <strong>доли</strong>. Для моделирования долей и распределений всегда использовалась статистика.</p>
<p><em>Интерпретация задач моделирования распределений как задачи классификации - это одна из самых распространенных ошибок, которые я видел в машинном обучении</em>. Сверхчеловеческая, идеальная модель распределения зачастую будет плохим классификатором.</p>
<p>Прекрасный пример этого - подбрасывание монеты. Если монета честная, и наша модель предсказывает вероятность выпадения орла ровно 0,5 без каких-либо неопределенностей, у нас есть идеальная модель поведения монеты. Но она же будет показывать ужасные результаты, если ее рассматривать как классификатор. Аналогично, представьте себе лотерею с шансом выигрыша в 1 / 1 000 000. Если ваша модель показывает точно и определенно шанс в 1 / 1 000 000, у вас прекрасная модель распределения. Но она будет отвратительным классификатором.</p>
<p>Даже если модели не работают как классификаторы, они могут дать нам ценную информацию для принятия решения в условиях неопределенности. Без правильной модели монеты мы можем принять неправильное решение о том, стоит ли ставить один доллар на монету, если при выпадении орла мы получим 2,1 доллара.</p>
<h3 id="проблема-предсказания">Проблема предсказания</h3>
<p>Но проблема лежит глубже простого непонимания разницы моделирования распределения и классификации. Ведь мы можем рассматривать результат работы модели еще до применения порога принятия решения. Мы могли бы придумать какую-нибудь метрику, точнее измеряющую результат работы модели.</p>
<p>Но даже с этими изменениями, в конечном итоге модель дает нам всего лишь одно предсказание, точечную оценку отклика при имеющейся у нас информации. Любой сведущий человек сразу же спросит: “Насколько мы уверены в этой оценке?” Этот вопрос особенно актуален, если мы занимаемся чем-то вроде оптимизации рекламных аукционов или выплат премий за неопределенный риск. На такой вопрос отвечает <strong>статистический вывод</strong>.</p>
<p>Но обращаясь к выводу, нам придется обратить более пристальное внимание на то, чему именно обучилась наша модель. Это важно, так как распределения не существует в данных априорно - это часть нашей модели. <strong>Для понимания задач распределения необходимо понимать саму модель, а не только ее результаты</strong>.</p>
<p>Для этого нам нужен статистический вывод.</p>
</section>
<footer class="page__meta">
<p class="page__taxonomy">
<strong><i aria-hidden="true" class="fas fa-fw fa-tags"></i> Метки: </strong>
<span itemprop="keywords">
<a class="page__taxonomy-item" href="/tags/#article" rel="tag">article</a><span class="sep">, </span>
<a class="page__taxonomy-item" href="/tags/#machine-learning" rel="tag">machine-learning</a><span class="sep">, </span>
<a class="page__taxonomy-item" href="/tags/#translation" rel="tag">translation</a>
</span>
</p>
<p class="page__taxonomy">
<strong><i aria-hidden="true" class="fas fa-fw fa-folder-open"></i> Разделы: </strong>
<span itemprop="keywords">
<a class="page__taxonomy-item" href="/categories/#scipop" rel="tag">scipop</a>
</span>
</p>
<p class="page__date"><strong><i aria-hidden="true" class="fas fa-fw fa-calendar-alt"></i> Дата изменения:</strong> <time datetime="2021-01-12T00:00:00+00:00">January 12, 2021</time></p>
</footer>
<nav class="pagination">
<a class="pagination--pager" href="/scipop/dider-prime-spirals/" title="Почему простые числа образуют спирали?
">Предыдущая</a>
<a class="pagination--pager" href="/scipop/inference-and-prediction-2/" title="Вывод и предсказания. Часть 2: статистика
">Следующая</a>
</nav>
</div>
</article>
<div class="page__related">
<h4 class="page__related-title">Вам также может понравиться</h4>
<div class="grid__wrapper">
<div class="grid__item">
<article class="archive__item" itemscope="" itemtype="https://schema.org/CreativeWork">
<div class="archive__item-teaser">
<img alt="" src="http://brightmagazine.ru/wp-content/uploads/2022/10/christina-wocintechchat-com-qZYNQp_Lm3o-unsplash.jpg"/>
</div>
<h2 class="archive__item-title no_toc" itemprop="headline">
<a href="http://brightmagazine.ru/ie/">Сколько получают IT-специалисты в России?
</a> <a href="/talks/bright-brains/" rel="permalink"><i aria-hidden="true" class="fas fa-link" title="permalink"></i><span class="sr-only">Permalink</span></a>
</h2>
<p class="page__meta">
<span class="page__meta-date">
<i aria-hidden="true" class="far fa-fw fa-calendar-alt"></i>
<time datetime="2022-09-26T00:00:00+00:00">September 26, 2022</time>
</span>
<span class="page__meta-sep"></span>
<span class="page__meta-readtime">
<i aria-hidden="true" class="far fa-fw fa-clock"></i>
        
          менее 1 мин на чтение
        
                
      </span>
</p>
<p class="archive__item-excerpt" itemprop="description">Вместе с заместителем декана по учебной работе Финансового университета при Правительстве РФ, кандидатом экономических наук, Михаилом Коротеевым разбираемся ...</p>
</article>
</div>
<div class="grid__item">
<article class="archive__item" itemscope="" itemtype="https://schema.org/CreativeWork">
<div class="archive__item-teaser">
<img alt="" src="https://brightmagazine.ru/wp-content/uploads/2022/05/pexels-george-morina-4960341.jpg"/>
</div>
<h2 class="archive__item-title no_toc" itemprop="headline">
<a href="https://brightmagazine.ru/proit/">Сколько получают IT-специалисты в России?
</a> <a href="/talks/bright-it-salaries/" rel="permalink"><i aria-hidden="true" class="fas fa-link" title="permalink"></i><span class="sr-only">Permalink</span></a>
</h2>
<p class="page__meta">
<span class="page__meta-date">
<i aria-hidden="true" class="far fa-fw fa-calendar-alt"></i>
<time datetime="2022-05-04T00:00:00+00:00">May 4, 2022</time>
</span>
<span class="page__meta-sep"></span>
<span class="page__meta-readtime">
<i aria-hidden="true" class="far fa-fw fa-clock"></i>
        
          менее 1 мин на чтение
        
                
      </span>
</p>
<p class="archive__item-excerpt" itemprop="description">Среди молодых специалистов России укоренился стереотип о том, что программисты, специалисты по ИТ «несправедливо» много зарабатывают. Насколько он оправдан, ...</p>
</article>
</div>
<div class="grid__item">
<article class="archive__item" itemscope="" itemtype="https://schema.org/CreativeWork">
<h2 class="archive__item-title no_toc" itemprop="headline">
<a href="https://www.youtube.com/playlist?list=PLhgyvraU60gUz40uPfZcCO6ar7dw7PCqs">Android-разработка
</a> <a href="/course/android-playlist/" rel="permalink"><i aria-hidden="true" class="fas fa-link" title="permalink"></i><span class="sr-only">Permalink</span></a>
</h2>
<p class="page__meta">
<span class="page__meta-date">
<i aria-hidden="true" class="far fa-fw fa-calendar-alt"></i>
<time datetime="2022-03-16T00:00:00+00:00">March 16, 2022</time>
</span>
<span class="page__meta-sep"></span>
<span class="page__meta-readtime">
<i aria-hidden="true" class="far fa-fw fa-clock"></i>
        
          менее 1 мин на чтение
        
                
      </span>
</p>
<p class="archive__item-excerpt" itemprop="description">Цикл видеолекций по разработке мобильных приложений для Android на языке Java.
</p>
</article>
</div>
<div class="grid__item">
<article class="archive__item" itemscope="" itemtype="https://schema.org/CreativeWork">
<h2 class="archive__item-title no_toc" itemprop="headline">
<a href="https://www.youtube.com/playlist?list=PLhgyvraU60gU8OAhjtcipU_sO7UYvkQl9">UNIX
</a> <a href="/course/linux-playlist/" rel="permalink"><i aria-hidden="true" class="fas fa-link" title="permalink"></i><span class="sr-only">Permalink</span></a>
</h2>
<p class="page__meta">
<span class="page__meta-date">
<i aria-hidden="true" class="far fa-fw fa-calendar-alt"></i>
<time datetime="2021-06-20T00:00:00+00:00">June 20, 2021</time>
</span>
<span class="page__meta-sep"></span>
<span class="page__meta-readtime">
<i aria-hidden="true" class="far fa-fw fa-clock"></i>
        
          менее 1 мин на чтение
        
                
      </span>
</p>
<p class="archive__item-excerpt" itemprop="description">Цикл видеолекций по операционной системе Linux.
</p>
</article>
</div>
</div>
</div>
</div>
</div>
<div class="page__footer" id="footer">
<footer>
<!-- start custom footer snippets -->
<!-- end custom footer snippets -->
<div class="page__footer-follow">
<ul class="social-icons">
<li><strong>Связаться со мной:</strong></li>
<li><a href="https://vk.com/koroteev_m" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fab fa-fw fa-vk"></i> Vkontakte</a></li>
<li><a href="https://twitter.com/koroteev_m" rel="nofollow noopener noreferrer"><i aria-hidden="true" class="fab fa-fw fa-twitter-square"></i> Twitter</a></li>
<li><a href="/feed.xml"><i aria-hidden="true" class="fas fa-fw fa-rss-square"></i> RSS-лента</a></li>
</ul>
</div>
<div class="page__footer-copyright">© 2022 Михаил Коротеев. Сайт работает на <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
</footer>
</div>
<script src="/assets/js/main.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1K09X3NDBE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1K09X3NDBE', { 'anonymize_ip': false});
</script>
<!-- Yandex.Metrika counter -->
<script type="text/javascript">
   (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
   m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
   (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

   ym(77706580, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
   });
</script>
<noscript><div><img alt="" src="https://mc.yandex.ru/watch/77706580" style="position:absolute; left:-9999px;"/></div></noscript>
<!-- /Yandex.Metrika counter -->
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true
     },
     "HTML-CSS": { availableFonts: ["TeX"] }
   });
</script>
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
</body>
</html>
